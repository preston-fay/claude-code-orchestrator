"""
AI Review Impact Metrics Collector

Tracks impact of AI code review comments from Phase 1A system:
- PRs with AI review comments
- Average AI suggestions per PR
- AI suggestion acceptance rate (heuristic: commits within 24h of review)
- Review response time

Data sources:
- GitHub PR comments (searches for AI review markers)
- Git commit history (correlates fixes to reviews)

Outputs:
- AI review impact metrics to .claude/metrics/ai_review/impact.json
"""

import os
import json
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import logging

try:
    from github import Github, GithubException
    from github.PullRequest import PullRequest

    GITHUB_AVAILABLE = True
except ImportError:
    GITHUB_AVAILABLE = False
    logging.warning("PyGithub not installed. Run: pip install PyGithub")

logger = logging.getLogger(__name__)


@dataclass
class AIReviewMetrics:
    """Metrics for AI code review impact"""

    pr_number: int
    pr_title: str
    review_date: str
    ai_comments_count: int
    issues_found: List[str] = field(default_factory=list)
    acceptance_indicators: List[str] = field(default_factory=list)
    response_time_hours: Optional[float] = None
    acceptance_rate_estimate: Optional[float] = None


@dataclass
class AIReviewImpactMetrics:
    """Overall AI review impact metrics"""

    metric: str = "ai_review_impact"
    period: str = ""
    collection_timestamp: str = ""
    reviews: List[Dict] = field(default_factory=list)
    summary: Dict = field(default_factory=dict)


class AIReviewImpactCollector:
    """Collect AI code review impact metrics from GitHub"""

    # AI review comment markers (from Phase 1A)
    AI_REVIEW_MARKERS = [
        "Claude Code Review",
        "AI Code Review",
        "Generated by Claude",
        "Reviewed by Claude",
        "[ai-review]",
        "automated code review",
    ]

    # Issue severity patterns
    ISSUE_PATTERNS = {
        "critical": [r"security.*vulnerability", r"data.*leak", r"critical.*bug"],
        "high": [r"memory.*leak", r"race.*condition", r"null.*pointer"],
        "medium": [r"code.*smell", r"complexity", r"maintainability"],
        "low": [r"style", r"formatting", r"naming.*convention"],
    }

    def __init__(
        self,
        repo_name: str,
        token: Optional[str] = None,
        days_back: int = 90,
        project_root: Optional[Path] = None,
    ):
        """
        Initialize AI review impact collector.

        Args:
            repo_name: Repository name in format 'owner/repo'
            token: GitHub API token (default: GITHUB_TOKEN env var)
            days_back: Number of days of history to analyze (default: 90)
            project_root: Root directory for output (default: current directory)
        """
        if not GITHUB_AVAILABLE:
            raise ImportError("PyGithub required. Install with: pip install PyGithub")

        self.repo_name = repo_name
        self.days_back = days_back
        self.token = token or os.getenv("GITHUB_TOKEN")

        if not self.token:
            raise ValueError(
                "GitHub token required. Set GITHUB_TOKEN environment variable or pass token parameter."
            )

        # Initialize GitHub client
        self.github = Github(self.token)
        self.repo = self.github.get_repo(repo_name)

        # Setup output directory
        if project_root:
            self.output_dir = Path(project_root) / ".claude" / "metrics" / "ai_review"
        else:
            self.output_dir = Path.cwd() / ".claude" / "metrics" / "ai_review"
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Initialized AI review impact collector for {repo_name}")

        # Log rate limit info
        try:
            rate_limit = self.github.get_rate_limit()
            if hasattr(rate_limit, "core"):
                logger.info(f"API rate limit: {rate_limit.core.remaining}/{rate_limit.core.limit}")
            else:
                logger.info(f"API rate limit: {rate_limit.rate.remaining}/{rate_limit.rate.limit}")
        except Exception as e:
            logger.debug(f"Could not fetch rate limit: {e}")

    def _is_ai_review_comment(self, comment_body: str) -> bool:
        """
        Check if a comment is from AI code review system.

        Args:
            comment_body: Comment text

        Returns:
            True if comment contains AI review markers
        """
        comment_lower = comment_body.lower()
        return any(marker.lower() in comment_lower for marker in self.AI_REVIEW_MARKERS)

    def _extract_issues(self, comment_body: str) -> List[str]:
        """
        Extract issues from AI review comment.

        Args:
            comment_body: Comment text

        Returns:
            List of issue descriptions
        """
        import re

        issues = []

        # Look for numbered list items
        numbered_pattern = r"^\s*\d+\.\s+(.+)$"
        for line in comment_body.split("\n"):
            match = re.match(numbered_pattern, line)
            if match:
                issues.append(match.group(1).strip())

        # Look for bullet points
        bullet_pattern = r"^\s*[-*]\s+(.+)$"
        for line in comment_body.split("\n"):
            match = re.match(bullet_pattern, line)
            if match:
                issue_text = match.group(1).strip()
                # Filter out common non-issue bullets
                if not any(
                    skip in issue_text.lower() for skip in ["summary", "overview", "note", "tip"]
                ):
                    issues.append(issue_text)

        # Look for severity indicators
        severity_pattern = r"(CRITICAL|HIGH|MEDIUM|LOW|WARNING|ERROR):\s*(.+)"
        for line in comment_body.split("\n"):
            match = re.search(severity_pattern, line, re.IGNORECASE)
            if match:
                issues.append(f"{match.group(1).upper()}: {match.group(2).strip()}")

        return issues

    def _calculate_acceptance_rate(
        self, pr: PullRequest, review_date: datetime, issues: List[str]
    ) -> Tuple[float, List[str]]:
        """
        Calculate heuristic acceptance rate for AI review suggestions.

        Heuristic: Check for commits within 24h of review that reference issues.

        Args:
            pr: GitHub PullRequest object
            review_date: Date of AI review comment
            issues: List of issues from review

        Returns:
            Tuple of (acceptance_rate_estimate, acceptance_indicators)
        """
        try:
            # Get commits after review
            commits = list(pr.get_commits())
            post_review_commits = [
                c
                for c in commits
                if c.commit.author.date > review_date
                and c.commit.author.date < review_date + timedelta(hours=24)
            ]

            if not post_review_commits:
                return 0.0, []

            # Check commit messages for issue references
            acceptance_indicators = []
            addressed_issues = 0

            for commit in post_review_commits:
                message_lower = commit.commit.message.lower()

                # Look for fix/address keywords
                if any(
                    keyword in message_lower
                    for keyword in ["fix", "address", "resolve", "refactor"]
                ):
                    acceptance_indicators.append(
                        f"Commit {commit.sha[:7]}: {commit.commit.message.split(chr(10))[0]}"
                    )
                    addressed_issues += 1

            # Estimate acceptance rate
            if issues:
                acceptance_rate = min((addressed_issues / len(issues)) * 100, 100)
            else:
                acceptance_rate = (
                    0.0 if not acceptance_indicators else 50.0
                )  # Default if no specific issues

            return acceptance_rate, acceptance_indicators

        except Exception as e:
            logger.warning(f"Could not calculate acceptance rate for PR #{pr.number}: {e}")
            return 0.0, []

    def _calculate_response_time(self, pr: PullRequest, review_date: datetime) -> Optional[float]:
        """
        Calculate time from AI review to first human response.

        Args:
            pr: GitHub PullRequest object
            review_date: Date of AI review comment

        Returns:
            Response time in hours, or None if no response
        """
        try:
            # Get comments after review
            comments = list(pr.get_issue_comments())
            post_review_comments = [
                c
                for c in comments
                if c.created_at > review_date and not self._is_ai_review_comment(c.body)
            ]

            if not post_review_comments:
                # Check for commits as response
                commits = list(pr.get_commits())
                post_review_commits = [c for c in commits if c.commit.author.date > review_date]

                if post_review_commits:
                    first_commit = post_review_commits[0]
                    response_time = (
                        first_commit.commit.author.date - review_date
                    ).total_seconds() / 3600
                    return round(response_time, 2)

                return None

            # Get first human comment
            first_comment = post_review_comments[0]
            response_time = (first_comment.created_at - review_date).total_seconds() / 3600
            return round(response_time, 2)

        except Exception as e:
            logger.warning(f"Could not calculate response time for PR #{pr.number}: {e}")
            return None

    def collect_review_impact(self) -> AIReviewImpactMetrics:
        """
        Collect AI code review impact metrics from GitHub PRs.

        Returns:
            AIReviewImpactMetrics object
        """
        logger.info(f"Collecting AI review impact data (last {self.days_back} days)")

        since_date = datetime.now(timezone.utc) - timedelta(days=self.days_back)

        # Get closed PRs
        prs = self.repo.get_pulls(state="closed", sort="updated", direction="desc")

        review_metrics = []
        total_prs_analyzed = 0
        prs_with_ai_reviews = 0
        total_ai_comments = 0
        total_issues_found = 0

        for pr in prs:
            # Filter by date
            if pr.updated_at < since_date:
                break

            total_prs_analyzed += 1

            # Get PR comments
            try:
                comments = list(pr.get_issue_comments())
            except Exception as e:
                logger.warning(f"Could not get comments for PR #{pr.number}: {e}")
                continue

            # Find AI review comments
            ai_comments = [c for c in comments if self._is_ai_review_comment(c.body)]

            if not ai_comments:
                continue

            prs_with_ai_reviews += 1
            total_ai_comments += len(ai_comments)

            # Process first AI review (most impactful)
            first_review = ai_comments[0]
            issues = self._extract_issues(first_review.body)
            total_issues_found += len(issues)

            # Calculate acceptance rate
            acceptance_rate, acceptance_indicators = self._calculate_acceptance_rate(
                pr, first_review.created_at, issues
            )

            # Calculate response time
            response_time = self._calculate_response_time(pr, first_review.created_at)

            # Build review metrics
            review_data = AIReviewMetrics(
                pr_number=pr.number,
                pr_title=pr.title,
                review_date=first_review.created_at.isoformat(),
                ai_comments_count=len(ai_comments),
                issues_found=issues,
                acceptance_indicators=acceptance_indicators,
                response_time_hours=response_time,
                acceptance_rate_estimate=acceptance_rate,
            )

            review_metrics.append(asdict(review_data))

            # Rate limit check
            if total_prs_analyzed % 10 == 0:
                try:
                    rate_limit = self.github.get_rate_limit()
                    remaining = (
                        rate_limit.core.remaining
                        if hasattr(rate_limit, "core")
                        else rate_limit.rate.remaining
                    )
                    if remaining < 100:
                        logger.warning(f"API rate limit low: {remaining} remaining")
                except Exception:
                    pass

        # Calculate summary statistics
        if review_metrics:
            avg_suggestions = total_issues_found / prs_with_ai_reviews
            avg_acceptance = sum(
                r["acceptance_rate_estimate"]
                for r in review_metrics
                if r["acceptance_rate_estimate"]
            ) / len(review_metrics)
            response_times = [
                r["response_time_hours"] for r in review_metrics if r["response_time_hours"]
            ]
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            review_coverage = (
                (prs_with_ai_reviews / total_prs_analyzed * 100) if total_prs_analyzed > 0 else 0
            )
        else:
            avg_suggestions = 0
            avg_acceptance = 0
            avg_response_time = 0
            review_coverage = 0

        # Build period string
        start_date = since_date.strftime("%Y-%m-%d")
        end_date = datetime.now().strftime("%Y-%m-%d")
        period = f"{start_date}/{end_date}"

        metrics = AIReviewImpactMetrics(
            period=period,
            collection_timestamp=datetime.now().isoformat(),
            reviews=review_metrics,
            summary={
                "total_prs_analyzed": total_prs_analyzed,
                "prs_with_ai_reviews": prs_with_ai_reviews,
                "review_coverage": round(review_coverage, 2),
                "total_ai_comments": total_ai_comments,
                "total_issues_found": total_issues_found,
                "avg_suggestions_per_pr": round(avg_suggestions, 2),
                "avg_acceptance_rate": round(avg_acceptance, 2),
                "avg_response_time_hours": round(avg_response_time, 2),
            },
        )

        # Save to JSON
        output_file = self.output_dir / "impact.json"
        with open(output_file, "w") as f:
            json.dump(asdict(metrics), f, indent=2)

        logger.info(
            f"Analyzed {total_prs_analyzed} PRs, found {prs_with_ai_reviews} with AI reviews"
        )
        logger.info(
            f"  Coverage: {review_coverage:.1f}%, Avg suggestions: {avg_suggestions:.1f}/PR"
        )
        logger.info(
            f"  Avg acceptance: {avg_acceptance:.1f}%, Avg response time: {avg_response_time:.1f}h"
        )

        return metrics


def main():
    """CLI entry point for AI review impact collection"""
    import argparse

    parser = argparse.ArgumentParser(description="Collect AI code review impact metrics")
    parser.add_argument(
        "--repo", type=str, required=True, help="Repository name in format owner/repo"
    )
    parser.add_argument(
        "--token", type=str, help="GitHub API token (default: GITHUB_TOKEN env var)"
    )
    parser.add_argument(
        "--days", type=int, default=90, help="Number of days of history to analyze (default: 90)"
    )
    parser.add_argument(
        "--output", type=Path, help="Output directory (default: .claude/metrics/ai_review/)"
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Setup logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # Run collection
    try:
        collector = AIReviewImpactCollector(
            repo_name=args.repo,
            token=args.token,
            days_back=args.days,
            project_root=args.output.parent.parent.parent if args.output else None,
        )

        collector.collect_review_impact()

    except Exception as e:
        logger.error(f"Collection failed: {e}")
        raise


if __name__ == "__main__":
    main()
