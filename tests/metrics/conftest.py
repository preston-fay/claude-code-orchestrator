"""
Pytest fixtures for metrics tests.

Provides mock data and test utilities for all metrics collectors.
"""

import pytest
import json
from pathlib import Path
from datetime import datetime, timedelta, timezone
from unittest.mock import Mock, MagicMock
from typing import List, Dict


@pytest.fixture
def temp_metrics_dir(tmp_path):
    """Create temporary metrics directory structure"""
    metrics_dir = tmp_path / ".claude" / "metrics"
    (metrics_dir / "dora").mkdir(parents=True)
    (metrics_dir / "github").mkdir(parents=True)
    (metrics_dir / "contributions").mkdir(parents=True)
    (metrics_dir / "ai_review").mkdir(parents=True)
    (metrics_dir / "aggregated").mkdir(parents=True)
    return metrics_dir


@pytest.fixture
def sample_git_log():
    """Sample git log output"""
    return """8f7674b97778cbff3374296b80069277e1203cf9|Preston Fay|2025-10-15 16:34:47 -0400|feat: complete Phase 13-B
acb1a8d1f97ae5e075c320319f1acef91680df11|Preston Fay|2025-11-07 10:11:50 -0500|feat: Phase 1A - AI Code Review system activated
6e6a89161cf3b4ff35d7e70883087e0fca9e11cd|Preston Fay|2025-10-27 09:17:41 -0400|feat: enable in-session guided execution"""


@pytest.fixture
def sample_git_tags():
    """Sample git tags output"""
    return """2025-10-15 16:34:47 -0400|8f7674b97778cbff3374296b80069277e1203cf9| (tag: v1.4.0)
2025-09-20 14:22:13 -0400|a1b2c3d4e5f6789012345678901234567890abcd| (tag: v1.3.0)
2025-08-15 10:10:10 -0400|b2c3d4e5f6789012345678901234567890abcdef| (tag: v1.2.0)"""


@pytest.fixture
def sample_commit_body():
    """Sample git commit body with Co-Authored-By"""
    return """feat: implement new feature

This is a detailed commit message.

Co-Authored-By: Claude <noreply@anthropic.com>
Generated with [Claude Code](https://claude.com/claude-code)"""


@pytest.fixture
def sample_commit_diff():
    """Sample git diff output"""
    return """diff --git a/src/test.py b/src/test.py
new file mode 100644
index 0000000..abc1234
--- /dev/null
+++ b/src/test.py
@@ -0,0 +1,5 @@
+# Generated by Claude
+def hello():
+    print("Hello, world!")
+
+hello()"""


@pytest.fixture
def mock_github_repo():
    """Mock GitHub repository"""
    repo = MagicMock()
    repo.full_name = "owner/repo"
    repo.default_branch = "main"
    return repo


@pytest.fixture
def mock_github_pr():
    """Mock GitHub pull request"""
    pr = MagicMock()
    pr.number = 123
    pr.title = "Test PR"
    pr.state = "closed"
    pr.merged = True
    pr.created_at = datetime(2025, 11, 1, 10, 0, 0, tzinfo=timezone.utc)
    pr.merged_at = datetime(2025, 11, 1, 14, 30, 0, tzinfo=timezone.utc)
    pr.closed_at = datetime(2025, 11, 1, 14, 30, 0, tzinfo=timezone.utc)
    pr.updated_at = datetime(2025, 11, 1, 14, 30, 0, tzinfo=timezone.utc)
    pr.commits = 5
    pr.changed_files = 3
    pr.additions = 150
    pr.deletions = 20
    pr.user = MagicMock()
    pr.user.login = "testuser"
    return pr


@pytest.fixture
def mock_github_comment():
    """Mock GitHub PR comment"""
    comment = MagicMock()
    comment.created_at = datetime(2025, 11, 1, 12, 0, 0, tzinfo=timezone.utc)
    comment.body = """Claude Code Review

Found 3 issues:
1. CRITICAL: Security vulnerability in authentication
2. HIGH: Memory leak in loop
3. MEDIUM: Code complexity too high"""
    return comment


@pytest.fixture
def mock_github_commit():
    """Mock GitHub commit"""
    commit = MagicMock()
    commit.sha = "abc123def456"
    commit.commit = MagicMock()
    commit.commit.author = MagicMock()
    commit.commit.author.date = datetime(2025, 11, 1, 13, 0, 0, tzinfo=timezone.utc)
    commit.commit.message = "fix: address code review findings"
    return commit


@pytest.fixture
def sample_dora_deployments():
    """Sample DORA deployment metrics"""
    return {
        "metric": "deployment_frequency",
        "period": "2025-10-08/2025-11-07",
        "collection_timestamp": "2025-11-07T10:34:12.240962",
        "deployments": [
            {
                "version": "v1.4.0",
                "timestamp": "2025-10-15T16:34:47",
                "commit_sha": "8f7674b97778cbff3374296b80069277e1203cf9",
                "branch": "main"
            }
        ],
        "summary": {
            "total_deployments": 1,
            "deploys_per_day": 0.05,
            "deploys_per_week": 0.32,
            "rating": "medium"
        }
    }


@pytest.fixture
def sample_contribution_metrics():
    """Sample contribution attribution metrics"""
    return {
        "metric": "contribution_attribution",
        "period": "2025-10-08/2025-11-07",
        "collection_timestamp": "2025-11-07T10:58:35.406891",
        "commits": [
            {
                "sha": "abc123",
                "author": "John Doe",
                "date": "2025-11-01 10:00:00 -0500",
                "message": "feat: add new feature",
                "contributor_type": "human",
                "lines_added": 100,
                "lines_deleted": 10,
                "files_changed": 3,
                "ai_indicators": []
            },
            {
                "sha": "def456",
                "author": "Jane Smith",
                "date": "2025-11-02 14:30:00 -0500",
                "message": "feat: collaborative feature",
                "contributor_type": "collaborative",
                "lines_added": 250,
                "lines_deleted": 20,
                "files_changed": 5,
                "ai_indicators": ["Co-Authored-By:.*Claude"]
            }
        ],
        "summary": {
            "total_commits": 2,
            "human_commits": 1,
            "ai_commits": 0,
            "collaborative_commits": 1,
            "human_percentage": 50.0,
            "ai_percentage": 0.0,
            "collaborative_percentage": 50.0,
            "human_lines_added": 100,
            "ai_lines_added": 0,
            "collaborative_lines_added": 250,
            "total_lines_added": 350,
            "human_lines_percentage": 28.57,
            "ai_lines_percentage": 0.0,
            "collaborative_lines_percentage": 71.43
        }
    }


@pytest.fixture
def sample_ai_review_metrics():
    """Sample AI review impact metrics"""
    return {
        "metric": "ai_review_impact",
        "period": "2025-10-08/2025-11-07",
        "collection_timestamp": "2025-11-07T11:00:13.407481",
        "reviews": [
            {
                "pr_number": 123,
                "pr_title": "Add authentication",
                "review_date": "2025-11-01T12:00:00",
                "ai_comments_count": 1,
                "issues_found": [
                    "CRITICAL: Security vulnerability",
                    "HIGH: Memory leak",
                    "MEDIUM: Code complexity"
                ],
                "acceptance_indicators": [
                    "Commit abc123: fix security issue"
                ],
                "response_time_hours": 2.5,
                "acceptance_rate_estimate": 66.67
            }
        ],
        "summary": {
            "total_prs_analyzed": 10,
            "prs_with_ai_reviews": 1,
            "review_coverage": 10.0,
            "total_ai_comments": 1,
            "total_issues_found": 3,
            "avg_suggestions_per_pr": 3.0,
            "avg_acceptance_rate": 66.67,
            "avg_response_time_hours": 2.5
        }
    }


@pytest.fixture
def mock_subprocess_run(monkeypatch):
    """Mock subprocess.run for git commands"""
    def mock_run(cmd, *args, **kwargs):
        result = Mock()
        result.stdout = ""
        result.stderr = ""
        result.returncode = 0

        # Return different data based on command
        if "log" in cmd and "--tags" in cmd:
            result.stdout = "2025-10-15 16:34:47 -0400|8f7674b|tag: v1.4.0"
        elif "log" in cmd:
            result.stdout = "abc123|John Doe|2025-11-01 10:00:00 -0400|feat: test"
        elif "show" in cmd and "--numstat" in cmd:
            result.stdout = "10\t5\tfile.py"

        return result

    monkeypatch.setattr("subprocess.run", mock_run)


@pytest.fixture
def write_sample_metrics(temp_metrics_dir, sample_dora_deployments, sample_contribution_metrics):
    """Write sample metrics files to temp directory"""
    # Write DORA metrics
    with open(temp_metrics_dir / "dora" / "deployments.json", 'w') as f:
        json.dump(sample_dora_deployments, f, indent=2)

    # Write contribution metrics
    with open(temp_metrics_dir / "contributions" / "attribution.json", 'w') as f:
        json.dump(sample_contribution_metrics, f, indent=2)

    return temp_metrics_dir
