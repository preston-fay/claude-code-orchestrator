"""
Unit tests for AI contribution analyzer.

Tests commit attribution and AI detection patterns.
"""

import pytest
import json
from unittest.mock import Mock, patch
from pathlib import Path

from src.orchestrator.metrics.contribution_analyzer import (
    AIContributionAnalyzer,
    CommitAttribution,
    ContributionMetrics
)


class TestAIContributionAnalyzer:
    """Test AIContributionAnalyzer class"""

    def test_init(self, temp_metrics_dir):
        """Test analyzer initialization"""
        analyzer = AIContributionAnalyzer(
            repo_path=Path.cwd(),
            days_back=30,
            project_root=temp_metrics_dir.parent.parent
        )

        assert analyzer.repo_path == Path.cwd()
        assert analyzer.days_back == 30
        assert analyzer.output_dir == temp_metrics_dir / "contributions"
        assert analyzer.output_dir.exists()

    def test_detect_ai_indicators(self, temp_metrics_dir):
        """Test AI indicator detection"""
        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)

        # Test Co-Authored-By Claude
        has_ai, indicators = analyzer._detect_ai_indicators(
            "feat: test",
            "Co-Authored-By: Claude <noreply@anthropic.com>"
        )
        assert has_ai is True
        assert len(indicators) > 0

        # Test [claude] marker
        has_ai, indicators = analyzer._detect_ai_indicators(
            "[claude] fix: bug",
            ""
        )
        assert has_ai is True

        # Test [ai] marker
        has_ai, indicators = analyzer._detect_ai_indicators(
            "[ai] feat: feature",
            ""
        )
        assert has_ai is True

        # Test Generated with Claude
        has_ai, indicators = analyzer._detect_ai_indicators(
            "feat: test",
            "Generated with [Claude Code](https://claude.com)"
        )
        assert has_ai is True

        # Test no indicators
        has_ai, indicators = analyzer._detect_ai_indicators(
            "feat: normal commit",
            "Just a regular commit message"
        )
        assert has_ai is False
        assert len(indicators) == 0

    def test_detect_ai_code_generation(self, temp_metrics_dir):
        """Test AI code generation signature detection"""
        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)

        # Test Python comment
        diff = """diff --git a/test.py b/test.py
+# Generated by Claude
+def test():
+    pass"""
        assert analyzer._detect_ai_code_generation(diff) is True

        # Test JavaScript comment
        diff = """diff --git a/test.js b/test.js
+// Generated by Claude
+function test() {}"""
        assert analyzer._detect_ai_code_generation(diff) is True

        # Test C-style comment
        diff = """diff --git a/test.c b/test.c
+/* Generated by Claude */
+int main() {}"""
        assert analyzer._detect_ai_code_generation(diff) is True

        # Test no signature
        diff = """diff --git a/test.py b/test.py
+def test():
+    pass"""
        assert analyzer._detect_ai_code_generation(diff) is False

    def test_classify_commit(self, temp_metrics_dir):
        """Test commit classification"""
        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)

        # Test human commit
        commit_data = {"message": "feat: normal feature"}
        commit_body = "Just a regular commit"
        diff = "def test():\n    pass"

        contributor_type, indicators = analyzer._classify_commit(commit_data, commit_body, diff)
        assert contributor_type == "human"
        assert len(indicators) == 0

        # Test collaborative commit (AI message indicator)
        commit_data = {"message": "feat: test"}
        commit_body = "Co-Authored-By: Claude <noreply@anthropic.com>"
        diff = "def test():\n    pass"

        contributor_type, indicators = analyzer._classify_commit(commit_data, commit_body, diff)
        assert contributor_type == "collaborative"
        assert len(indicators) > 0

        # Test AI commit (code generation signature only)
        commit_data = {"message": "feat: test"}
        commit_body = "Regular message"
        diff = "# Generated by Claude\ndef test():\n    pass"

        contributor_type, indicators = analyzer._classify_commit(commit_data, commit_body, diff)
        assert contributor_type == "ai"
        assert "AI code generation signature detected" in indicators

        # Test collaborative commit (both indicators)
        commit_data = {"message": "[claude] feat: test"}
        commit_body = "Regular message"
        diff = "# Generated by Claude\ndef test():\n    pass"

        contributor_type, indicators = analyzer._classify_commit(commit_data, commit_body, diff)
        assert contributor_type == "collaborative"
        assert len(indicators) >= 2

    @patch('subprocess.run')
    def test_parse_commit_stats(self, mock_run, temp_metrics_dir):
        """Test commit statistics parsing"""
        mock_run.return_value = Mock(
            stdout="10\t5\tfile1.py\n20\t3\tfile2.py\n-\t-\tbinary.bin",
            returncode=0
        )

        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)
        lines_added, lines_deleted, files_changed = analyzer._parse_commit_stats("abc123")

        assert lines_added == 30  # 10 + 20
        assert lines_deleted == 8  # 5 + 3
        assert files_changed == 3  # file1.py, file2.py, binary.bin

    @patch('subprocess.run')
    def test_analyze_contributions(self, mock_run, temp_metrics_dir):
        """Test full contribution analysis"""
        def mock_run_side_effect(cmd, *args, **kwargs):
            result = Mock(returncode=0)

            if "log" in cmd and "--format" in cmd:
                result.stdout = """abc123|John Doe|2025-11-01 10:00:00 -0400|feat: human commit
def456|Jane Smith|2025-11-02 14:30:00 -0400|feat: collaborative feature"""
            elif "show" in cmd and "--format=%B" in cmd:
                if "abc123" in cmd:
                    result.stdout = "Just a regular commit"
                else:
                    result.stdout = "Co-Authored-By: Claude <noreply@anthropic.com>"
            elif "show" in cmd and "--format=" in cmd:
                result.stdout = "def test():\n    pass"
            elif "show" in cmd and "--numstat" in cmd:
                result.stdout = "100\t10\tfile.py"
            else:
                result.stdout = ""

            return result

        mock_run.side_effect = mock_run_side_effect

        analyzer = AIContributionAnalyzer(days_back=30, project_root=temp_metrics_dir.parent.parent)
        metrics = analyzer.analyze_contributions()

        assert metrics.metric == "contribution_attribution"
        assert metrics.summary["total_commits"] == 2
        assert metrics.summary["human_commits"] >= 0
        assert metrics.summary["collaborative_commits"] >= 0

    @patch('subprocess.run')
    def test_analyze_contributions_empty(self, mock_run, temp_metrics_dir):
        """Test analysis with no commits"""
        mock_run.return_value = Mock(stdout="", returncode=0)

        analyzer = AIContributionAnalyzer(days_back=30, project_root=temp_metrics_dir.parent.parent)
        metrics = analyzer.analyze_contributions()

        assert metrics.summary["total_commits"] == 0
        assert metrics.summary["human_percentage"] == 0
        assert metrics.summary["ai_percentage"] == 0

    @patch('subprocess.run')
    def test_json_output(self, mock_run, temp_metrics_dir):
        """Test JSON file output"""
        mock_run.return_value = Mock(
            stdout="abc123|John Doe|2025-11-01 10:00:00 -0400|feat: test",
            returncode=0
        )

        analyzer = AIContributionAnalyzer(days_back=30, project_root=temp_metrics_dir.parent.parent)
        analyzer.analyze_contributions()

        # Check JSON file was created
        output_file = temp_metrics_dir / "contributions" / "attribution.json"
        assert output_file.exists()

        # Validate JSON structure
        with open(output_file, 'r') as f:
            data = json.load(f)

        assert data["metric"] == "contribution_attribution"
        assert "period" in data
        assert "commits" in data
        assert "summary" in data

    def test_ai_patterns_comprehensive(self, temp_metrics_dir):
        """Test all AI detection patterns"""
        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)

        patterns_to_test = [
            "Co-Authored-By: Claude <noreply@anthropic.com>",
            "[claude] feat: test",
            "[ai] fix: bug",
            "[ai-generated] chore: update",
            "Generated with Claude Code",
            "Assisted-by: Claude"
        ]

        for pattern in patterns_to_test:
            has_ai, indicators = analyzer._detect_ai_indicators("test", pattern)
            assert has_ai is True, f"Pattern '{pattern}' should be detected"
            assert len(indicators) > 0

    def test_percentage_calculations(self, temp_metrics_dir):
        """Test percentage calculation accuracy"""
        analyzer = AIContributionAnalyzer(project_root=temp_metrics_dir.parent.parent)

        # Create mock metrics
        total_commits = 10
        human_commits = 3
        ai_commits = 2
        collaborative_commits = 5

        human_pct = (human_commits / total_commits * 100)
        ai_pct = (ai_commits / total_commits * 100)
        collab_pct = (collaborative_commits / total_commits * 100)

        assert human_pct == 30.0
        assert ai_pct == 20.0
        assert collab_pct == 50.0
        assert human_pct + ai_pct + collab_pct == 100.0
